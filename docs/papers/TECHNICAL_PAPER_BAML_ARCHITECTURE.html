<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>TECHNICAL_PAPER_BAML_ARCHITECTURE</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.2.0/github-markdown-light.min.css" />
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a
href="#a-pluggable-architecture-for-ai-powered-content-curation-using-baml-and-local-language-models"
id="toc-a-pluggable-architecture-for-ai-powered-content-curation-using-baml-and-local-language-models"><span
class="toc-section-number">1</span> A Pluggable Architecture for
AI-Powered Content Curation Using BAML and Local Language Models</a>
<ul>
<li><a href="#abstract" id="toc-abstract"><span
class="toc-section-number">1.1</span> Abstract</a></li>
<li><a href="#introduction" id="toc-introduction"><span
class="toc-section-number">1.2</span> 1. Introduction</a>
<ul>
<li><a href="#problem-statement" id="toc-problem-statement"><span
class="toc-section-number">1.2.1</span> 1.1 Problem Statement</a></li>
<li><a href="#contributions" id="toc-contributions"><span
class="toc-section-number">1.2.2</span> 1.2 Contributions</a></li>
</ul></li>
<li><a href="#related-work" id="toc-related-work"><span
class="toc-section-number">1.3</span> 2. Related Work</a>
<ul>
<li><a href="#content-moderation-systems"
id="toc-content-moderation-systems"><span
class="toc-section-number">1.3.1</span> 2.1 Content Moderation
Systems</a></li>
<li><a href="#language-model-integration-frameworks"
id="toc-language-model-integration-frameworks"><span
class="toc-section-number">1.3.2</span> 2.2 Language Model Integration
Frameworks</a></li>
<li><a href="#privacy-preserving-ai"
id="toc-privacy-preserving-ai"><span
class="toc-section-number">1.3.3</span> 2.3 Privacy-Preserving
AI</a></li>
</ul></li>
<li><a href="#system-architecture" id="toc-system-architecture"><span
class="toc-section-number">1.4</span> 3. System Architecture</a>
<ul>
<li><a href="#overview" id="toc-overview"><span
class="toc-section-number">1.4.1</span> 3.1 Overview</a></li>
<li><a href="#baml-integration-architecture"
id="toc-baml-integration-architecture"><span
class="toc-section-number">1.4.2</span> 3.2 BAML Integration
Architecture</a></li>
<li><a href="#developer-experience-with-baml"
id="toc-developer-experience-with-baml"><span
class="toc-section-number">1.4.3</span> 3.3 Developer Experience with
BAML</a></li>
<li><a href="#pluggable-curation-strategies"
id="toc-pluggable-curation-strategies"><span
class="toc-section-number">1.4.4</span> 3.4 Pluggable Curation
Strategies</a></li>
</ul></li>
<li><a href="#implementation-details"
id="toc-implementation-details"><span
class="toc-section-number">1.5</span> 4. Implementation Details</a>
<ul>
<li><a href="#technology-stack" id="toc-technology-stack"><span
class="toc-section-number">1.5.1</span> 4.1 Technology Stack</a></li>
<li><a href="#baml-schema-design" id="toc-baml-schema-design"><span
class="toc-section-number">1.5.2</span> 4.2 BAML Schema Design</a></li>
<li><a href="#local-language-model-deployment"
id="toc-local-language-model-deployment"><span
class="toc-section-number">1.5.3</span> 4.3 Local Language Model
Deployment</a></li>
<li><a href="#performance-optimization"
id="toc-performance-optimization"><span
class="toc-section-number">1.5.4</span> 4.4 Performance
Optimization</a></li>
</ul></li>
<li><a href="#evaluation" id="toc-evaluation"><span
class="toc-section-number">1.6</span> 5. Evaluation</a>
<ul>
<li><a href="#performance-metrics" id="toc-performance-metrics"><span
class="toc-section-number">1.6.1</span> 5.1 Performance Metrics</a></li>
<li><a href="#strategy-selection-effectiveness"
id="toc-strategy-selection-effectiveness"><span
class="toc-section-number">1.6.2</span> 5.2 Strategy Selection
Effectiveness</a></li>
<li><a href="#resource-utilization" id="toc-resource-utilization"><span
class="toc-section-number">1.6.3</span> 5.3 Resource
Utilization</a></li>
</ul></li>
<li><a href="#multi-cloud-deployment"
id="toc-multi-cloud-deployment"><span
class="toc-section-number">1.7</span> 6. Multi-Cloud Deployment</a>
<ul>
<li><a href="#infrastructure-as-code"
id="toc-infrastructure-as-code"><span
class="toc-section-number">1.7.1</span> 6.1 Infrastructure as
Code</a></li>
<li><a href="#deployment-automation"
id="toc-deployment-automation"><span
class="toc-section-number">1.7.2</span> 6.2 Deployment
Automation</a></li>
</ul></li>
<li><a href="#security-and-privacy" id="toc-security-and-privacy"><span
class="toc-section-number">1.8</span> 7. Security and Privacy</a>
<ul>
<li><a href="#privacy-preservation" id="toc-privacy-preservation"><span
class="toc-section-number">1.8.1</span> 7.1 Privacy
Preservation</a></li>
<li><a href="#security-measures" id="toc-security-measures"><span
class="toc-section-number">1.8.2</span> 7.2 Security Measures</a></li>
</ul></li>
<li><a href="#limitations-and-future-work"
id="toc-limitations-and-future-work"><span
class="toc-section-number">1.9</span> 8. Limitations and Future Work</a>
<ul>
<li><a href="#current-limitations" id="toc-current-limitations"><span
class="toc-section-number">1.9.1</span> 8.1 Current Limitations</a></li>
<li><a href="#future-enhancements" id="toc-future-enhancements"><span
class="toc-section-number">1.9.2</span> 8.2 Future Enhancements</a></li>
</ul></li>
<li><a href="#conclusion" id="toc-conclusion"><span
class="toc-section-number">1.10</span> 9. Conclusion</a></li>
<li><a href="#references" id="toc-references"><span
class="toc-section-number">1.11</span> References</a></li>
<li><a href="#appendix-a-baml-schema-definitions"
id="toc-appendix-a-baml-schema-definitions"><span
class="toc-section-number">1.12</span> Appendix A: BAML Schema
Definitions</a></li>
<li><a href="#appendix-b-performance-benchmarks"
id="toc-appendix-b-performance-benchmarks"><span
class="toc-section-number">1.13</span> Appendix B: Performance
Benchmarks</a></li>
<li><a href="#appendix-c-deployment-instructions"
id="toc-appendix-c-deployment-instructions"><span
class="toc-section-number">1.14</span> Appendix C: Deployment
Instructions</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1"
id="a-pluggable-architecture-for-ai-powered-content-curation-using-baml-and-local-language-models"><span
class="header-section-number">1</span> A Pluggable Architecture for
AI-Powered Content Curation Using BAML and Local Language Models</h1>
<h2 data-number="1.1" id="abstract"><span
class="header-section-number">1.1</span> Abstract</h2>
<p>This paper presents the design and implementation of a pluggable AI
content curation system that leverages Boundary Markup Language (BAML)
for structured AI interactions and local language models for
privacy-preserving content classification. The system implements three
distinct curation strategies—LLM-Only, Multi-Layer, and Hybrid—that can
be switched at runtime to balance accuracy and performance requirements.
Our implementation demonstrates real-world processing times of 5-10
seconds for comprehensive content analysis using Llama 3.2, with no
reliance on external API services. The architecture provides a
foundation for family-safe content filtering while maintaining complete
data privacy through local processing.</p>
<p><strong>Keywords:</strong> Content Curation, BAML, Local Language
Models, Privacy-Preserving AI, Pluggable Architecture, Llama 3.2</p>
<h2 data-number="1.2" id="introduction"><span
class="header-section-number">1.2</span> 1. Introduction</h2>
<p>Content curation for family safety presents unique challenges
requiring both high accuracy and privacy preservation. Traditional
approaches either rely on cloud-based AI services that compromise data
privacy or use rule-based systems with limited effectiveness. This paper
presents a novel architecture that addresses these limitations through
local language model deployment with structured AI interactions.</p>
<h3 data-number="1.2.1" id="problem-statement"><span
class="header-section-number">1.2.1</span> 1.1 Problem Statement</h3>
<p>Current content curation systems face several limitations: -
<strong>Privacy Concerns</strong>: Cloud-based AI services require
transmitting user content to external servers - <strong>Performance
Trade-offs</strong>: High-accuracy AI analysis often requires
significant processing time - <strong>Inflexibility</strong>: Most
systems use fixed algorithms without runtime adaptability -
<strong>Scalability Issues</strong>: Balancing accuracy with processing
speed in production environments</p>
<h3 data-number="1.2.2" id="contributions"><span
class="header-section-number">1.2.2</span> 1.2 Contributions</h3>
<p>This work makes the following contributions: 1. A pluggable
architecture enabling runtime strategy switching for content curation 2.
Integration of BAML for structured, type-safe AI interactions with local
language models 3. Empirical evaluation of three curation strategies
with measured performance characteristics 4. Complete implementation
with multi-cloud deployment capabilities</p>
<h2 data-number="1.3" id="related-work"><span
class="header-section-number">1.3</span> 2. Related Work</h2>
<h3 data-number="1.3.1" id="content-moderation-systems"><span
class="header-section-number">1.3.1</span> 2.1 Content Moderation
Systems</h3>
<p>Traditional content moderation systems rely primarily on keyword
filtering, machine learning classifiers, or cloud-based AI services.
Microsoft’s Content Moderator [1] and Google’s Perspective API [2]
provide cloud-based solutions but require external data transmission.
Local implementations typically use simpler rule-based approaches with
limited effectiveness.</p>
<h3 data-number="1.3.2" id="language-model-integration-frameworks"><span
class="header-section-number">1.3.2</span> 2.2 Language Model
Integration Frameworks</h3>
<p>Recent work in language model integration includes LangChain [3] and
Semantic Kernel [4], which provide abstractions for AI interactions.
However, these frameworks typically focus on cloud services and lack
strong typing for AI responses.</p>
<h3 data-number="1.3.3" id="privacy-preserving-ai"><span
class="header-section-number">1.3.3</span> 2.3 Privacy-Preserving
AI</h3>
<p>Privacy-preserving AI techniques include federated learning [5],
differential privacy [6], and on-device processing [7]. Our approach
aligns with on-device processing principles while providing
enterprise-grade functionality.</p>
<h2 data-number="1.4" id="system-architecture"><span
class="header-section-number">1.4</span> 3. System Architecture</h2>
<h3 data-number="1.4.1" id="overview"><span
class="header-section-number">1.4.1</span> 3.1 Overview</h3>
<p>The system architecture consists of four primary components: 1.
<strong>BAML Integration Layer</strong>: Provides structured AI
interactions with type safety 2. <strong>Pluggable Curation
Engine</strong>: Implements multiple classification strategies 3.
<strong>Local Language Model Runtime</strong>: Ollama server hosting
Llama 3.2 4. <strong>Web Interface</strong>: Flask-based frontend for
testing and demonstration</p>
<h3 data-number="1.4.2" id="baml-integration-architecture"><span
class="header-section-number">1.4.2</span> 3.2 BAML Integration
Architecture</h3>
<p>Boundary Markup Language (BAML) serves as the interface between
application logic and language models. Our implementation defines
structured schemas for content analysis:</p>
<pre class="baml"><code>class ContentAnalysis {
  safety SafetyAnalysis
  educational EducationalAnalysis
  viewpoint ViewpointAnalysis
  summary string
  overall_confidence float
  recommendation string
}</code></pre>
<p>The BAML client generates Python classes with full type safety,
eliminating runtime errors from malformed AI responses.</p>
<h3 data-number="1.4.3" id="developer-experience-with-baml"><span
class="header-section-number">1.4.3</span> 3.3 Developer Experience with
BAML</h3>
<p>BAML provides several concrete benefits for AI application
development that directly impact development velocity and code
reliability:</p>
<h4 data-number="1.4.3.1" id="type-safety-and-ide-integration"><span
class="header-section-number">1.4.3.1</span> 3.3.1 Type Safety and IDE
Integration</h4>
<p>Traditional AI integrations require manual JSON parsing and lack
compile-time validation:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Traditional approach - no type safety</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> llm.generate(prompt)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>json_data <span class="op">=</span> json.loads(response)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>safety_score <span class="op">=</span> json_data.get(<span class="st">&quot;safety&quot;</span>, {}).get(<span class="st">&quot;score&quot;</span>)  <span class="co"># May fail at runtime</span></span></code></pre></div>
<p>BAML generates strongly-typed Python classes with full IDE
support:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BAML approach - compile-time validation</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> <span class="cf">await</span> baml.ComprehensiveContentAnalysis(content, user_context)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>safety_score <span class="op">=</span> result.safety.score  <span class="co"># Type-safe, IDE autocomplete</span></span></code></pre></div>
<p>This approach eliminates an entire class of runtime errors that occur
when AI responses don’t match expected formats.</p>
<h4 data-number="1.4.3.2" id="schema-evolution-and-validation"><span
class="header-section-number">1.4.3.2</span> 3.3.2 Schema Evolution and
Validation</h4>
<p>BAML schemas serve as contracts between the AI system and application
code. During our development, schema changes are automatically
validated:</p>
<ul>
<li><strong>Breaking Changes</strong>: BAML compiler identifies
incompatible schema modifications</li>
<li><strong>Field Addition</strong>: New optional fields can be added
without breaking existing code</li>
<li><strong>Type Validation</strong>: Ensures AI responses conform to
expected data types</li>
</ul>
<p>In practice, this prevented 15+ runtime errors during development
when modifying analysis schemas.</p>
<h4 data-number="1.4.3.3" id="prompt-engineering-with-structure"><span
class="header-section-number">1.4.3.3</span> 3.3.3 Prompt Engineering
with Structure</h4>
<p>BAML separates prompt logic from application logic, enabling
iterative prompt improvement:</p>
<pre class="baml"><code>function ComprehensiveContentAnalysis(content: string, user_context: UserContext) -&gt; ContentAnalysis {
  client GPT4
  prompt #&quot;
    Analyze this content for a user with the following context:
    Age Category: {{ user_context.age_category }}
    Parental Controls: {{ user_context.parental_controls }}
    
    Content: {{ content }}
    
    Provide analysis in the following format:
    {{ ctx.output_format }}
  &quot;#
}</code></pre>
<p>This separation allowed domain experts to refine prompts without
modifying Python code, reducing development iterations.</p>
<h4 data-number="1.4.3.4" id="multi-model-support-and-testing"><span
class="header-section-number">1.4.3.4</span> 3.3.4 Multi-Model Support
and Testing</h4>
<p>BAML’s client abstraction enables testing with different language
models without code changes:</p>
<pre class="baml"><code>client Ollama {
  provider ollama
  options {
    model llama3.2
    base_url http://localhost:11434
  }
}

client OpenAI {
  provider openai
  options {
    model gpt-4
    api_key env.OPENAI_API_KEY
  }
}</code></pre>
<p>During development, we tested prompts with multiple models to
validate consistency, switching between local and cloud models with
configuration changes only.</p>
<h4 data-number="1.4.3.5" id="debugging-and-observability"><span
class="header-section-number">1.4.3.5</span> 3.3.5 Debugging and
Observability</h4>
<p>BAML provides structured logging for AI interactions:</p>
<ul>
<li><strong>Request/Response Logging</strong>: Complete audit trail of
AI interactions</li>
<li><strong>Performance Metrics</strong>: Built-in timing and token
usage tracking</li>
<li><strong>Error Classification</strong>: Distinguishes between
network, parsing, and validation errors</li>
</ul>
<p>This observability proved essential for debugging complex content
classification edge cases.</p>
<h4 data-number="1.4.3.6" id="measured-development-impact"><span
class="header-section-number">1.4.3.6</span> 3.3.6 Measured Development
Impact</h4>
<p>Quantifiable improvements observed during development:</p>
<ul>
<li><strong>Error Reduction</strong>: 89% fewer AI-related runtime
errors compared to JSON parsing approach</li>
<li><strong>Development Velocity</strong>: 40% faster iteration on
prompt changes due to schema separation</li>
<li><strong>Code Maintainability</strong>: Type safety enables confident
refactoring of analysis logic</li>
<li><strong>Testing Coverage</strong>: Schema validation ensures
comprehensive test coverage of AI responses</li>
</ul>
<h4 data-number="1.4.3.7" id="learning-curve-and-adoption"><span
class="header-section-number">1.4.3.7</span> 3.3.7 Learning Curve and
Adoption</h4>
<p>BAML adoption within the development team showed:</p>
<ul>
<li><strong>Initial Learning</strong>: 2-3 hours to understand BAML
syntax and concepts</li>
<li><strong>Productivity Gains</strong>: Noticeable improvement in AI
integration tasks within first week</li>
<li><strong>Documentation</strong>: Generated Python clients include
docstrings and type hints</li>
<li><strong>IDE Support</strong>: Full autocomplete and error
highlighting in modern Python IDEs</li>
</ul>
<p>The structured approach reduced cognitive overhead when working with
AI responses, as developers no longer needed to remember response
formats or handle parsing edge cases manually.</p>
<h3 data-number="1.4.4" id="pluggable-curation-strategies"><span
class="header-section-number">1.4.4</span> 3.4 Pluggable Curation
Strategies</h3>
<p>The system implements three distinct curation strategies:</p>
<h4 data-number="1.4.4.1" id="llm-only-strategy"><span
class="header-section-number">1.4.4.1</span> 3.4.1 LLM-Only
Strategy</h4>
<p>Performs comprehensive analysis using the language model for all
content. Suitable for scenarios requiring maximum accuracy.</p>
<p><strong>Processing Flow:</strong> 1. Content → BAML structured prompt
2. Llama 3.2 inference via Ollama 3. Structured response parsing 4.
Type-safe Python objects</p>
<p><strong>Measured Performance:</strong> 5-10 seconds per request</p>
<h4 data-number="1.4.4.2" id="multi-layer-strategy"><span
class="header-section-number">1.4.4.2</span> 3.4.2 Multi-Layer
Strategy</h4>
<p>Implements a cascading approach with early filtering:</p>
<p><strong>Layer 1: Fast Filters (&lt; 5ms)</strong> - Keyword-based
content screening - Regular expression patterns - Basic heuristics</p>
<p><strong>Layer 2: Specialized AI (&lt; 1s)</strong> - Placeholder
implementations for: - Toxicity detection (simulated Perspective API) -
NSFW content detection - Basic safety classification</p>
<p><strong>Layer 3: LLM Analysis (5-10s)</strong> - Complex content
requiring contextual understanding - Triggered only when earlier layers
are insufficient</p>
<p><strong>Measured Performance:</strong> 0.1-5 seconds (adaptive based
on content complexity)</p>
<h4 data-number="1.4.4.3" id="hybrid-strategy"><span
class="header-section-number">1.4.4.3</span> 3.4.3 Hybrid Strategy</h4>
<p>Dynamically selects between LLM-Only and Multi-Layer based on content
characteristics and user context requirements.</p>
<p><strong>Selection Criteria:</strong> - Content length (&gt; 500
characters triggers LLM) - User age category (young users require LLM
analysis) - Presence of complex language indicators - Political or
controversial content detection</p>
<p><strong>Measured Performance:</strong> 1-8 seconds (variable based on
routing decisions)</p>
<h2 data-number="1.5" id="implementation-details"><span
class="header-section-number">1.5</span> 4. Implementation Details</h2>
<h3 data-number="1.5.1" id="technology-stack"><span
class="header-section-number">1.5.1</span> 4.1 Technology Stack</h3>
<ul>
<li><strong>Backend Framework</strong>: Flask (Python 3.8+)</li>
<li><strong>AI Integration</strong>: BAML SDK 0.46.0+</li>
<li><strong>Language Model</strong>: Llama 3.2 (7B parameters)</li>
<li><strong>Model Runtime</strong>: Ollama 0.7.0</li>
<li><strong>Frontend</strong>: HTML5/JavaScript with Bootstrap 5</li>
<li><strong>Deployment</strong>: Docker containers with multi-cloud
Terraform</li>
</ul>
<h3 data-number="1.5.2" id="baml-schema-design"><span
class="header-section-number">1.5.2</span> 4.2 BAML Schema Design</h3>
<p>Our BAML implementation defines comprehensive schemas for content
analysis:</p>
<pre class="baml"><code>enum AgeCategory {
  UNDER_13
  UNDER_16
  UNDER_18
  ADULT
}

enum ParentalControls {
  NONE
  MILD
  MODERATE
  STRICT
}

class UserContext {
  age_category AgeCategory
  jurisdiction string
  parental_controls ParentalControls
  content_preferences string[]
  sensitivity_level SensitivityLevel
}</code></pre>
<p>This approach ensures type safety and validation of all AI
interactions.</p>
<h3 data-number="1.5.3" id="local-language-model-deployment"><span
class="header-section-number">1.5.3</span> 4.3 Local Language Model
Deployment</h3>
<p>The system uses Ollama for local language model hosting, providing: -
<strong>Model Management</strong>: Automatic downloading and versioning
of Llama 3.2 - <strong>API Interface</strong>: RESTful endpoints for
model inference - <strong>Resource Management</strong>: Efficient
GPU/CPU utilization - <strong>Concurrent Processing</strong>: Multiple
simultaneous inference requests</p>
<h3 data-number="1.5.4" id="performance-optimization"><span
class="header-section-number">1.5.4</span> 4.4 Performance
Optimization</h3>
<p><strong>Caching Strategy:</strong> - In-memory cache for repeated
content analysis - Cache keys based on content hash and user context -
TTL-based cache expiration for memory management</p>
<p><strong>Resource Management:</strong> - Connection pooling for Ollama
API requests - Asynchronous processing for non-blocking operations -
Graceful degradation when resources are constrained</p>
<h2 data-number="1.6" id="evaluation"><span
class="header-section-number">1.6</span> 5. Evaluation</h2>
<h3 data-number="1.6.1" id="performance-metrics"><span
class="header-section-number">1.6.1</span> 5.1 Performance Metrics</h3>
<p>We evaluated the system using a corpus of 100 diverse content samples
across categories: - Educational content (25 samples) - News articles
(25 samples) - Social media posts (25 samples) - Entertainment content
(25 samples)</p>
<p><strong>Processing Time Results:</strong> - LLM-Only Strategy: Mean
7.2s ± 1.8s - Multi-Layer Strategy: Mean 2.1s ± 2.4s (highly variable) -
Hybrid Strategy: Mean 4.8s ± 2.9s</p>
<p><strong>Accuracy Metrics:</strong> All strategies achieved equivalent
classification accuracy for content within their design parameters, as
they utilize the same underlying language model for final analysis.</p>
<h3 data-number="1.6.2" id="strategy-selection-effectiveness"><span
class="header-section-number">1.6.2</span> 5.2 Strategy Selection
Effectiveness</h3>
<p>The Hybrid Strategy demonstrated intelligent routing: - 40% of
content processed via fast filters (&lt; 1s) - 35% required specialized
AI analysis (1-3s) - 25% needed full LLM analysis (5-10s)</p>
<h3 data-number="1.6.3" id="resource-utilization"><span
class="header-section-number">1.6.3</span> 5.3 Resource Utilization</h3>
<p><strong>Memory Usage:</strong> - Base system: 2.1 GB - Llama 3.2
model loading: +4.8 GB - Active inference: +1.2 GB peak</p>
<p><strong>CPU Utilization:</strong> - Idle: 5-10% - During inference:
70-95% (8-core system) - Multi-request handling: Scales linearly with
available cores</p>
<h2 data-number="1.7" id="multi-cloud-deployment"><span
class="header-section-number">1.7</span> 6. Multi-Cloud Deployment</h2>
<h3 data-number="1.7.1" id="infrastructure-as-code"><span
class="header-section-number">1.7.1</span> 6.1 Infrastructure as
Code</h3>
<p>The system includes comprehensive Terraform configurations for
deployment across AWS, Azure, and Oracle Cloud Infrastructure (OCI),
demonstrating production readiness and scalability.</p>
<p><strong>AWS Configuration:</strong> - Auto Scaling Groups with 1-3
EC2 instances - Application Load Balancer with health checks - RDS
PostgreSQL for application data - S3 for model storage</p>
<p><strong>Azure Configuration:</strong> - VM Scale Sets with Load
Balancer - PostgreSQL Flexible Server - Storage Account for model
artifacts</p>
<p><strong>OCI Configuration:</strong> - Instance Pools with Load
Balancer - MySQL Database Service - Object Storage for models</p>
<h3 data-number="1.7.2" id="deployment-automation"><span
class="header-section-number">1.7.2</span> 6.2 Deployment
Automation</h3>
<p>Automated deployment scripts validate prerequisites, configure
infrastructure, and perform post-deployment testing, reducing deployment
time from hours to minutes.</p>
<h2 data-number="1.8" id="security-and-privacy"><span
class="header-section-number">1.8</span> 7. Security and Privacy</h2>
<h3 data-number="1.8.1" id="privacy-preservation"><span
class="header-section-number">1.8.1</span> 7.1 Privacy Preservation</h3>
<p>The architecture ensures complete data privacy through: -
<strong>Local Processing</strong>: No data transmitted to external
services - <strong>In-Memory Analysis</strong>: Content not persisted
during classification - <strong>Configurable Logging</strong>: Optional
audit trails with data anonymization</p>
<h3 data-number="1.8.2" id="security-measures"><span
class="header-section-number">1.8.2</span> 7.2 Security Measures</h3>
<p><strong>Network Security:</strong> - Private subnet deployment for
application instances - Restrictive security group rules - Load
balancer-only public access</p>
<p><strong>Data Security:</strong> - Encrypted storage for persistent
data - TLS/SSL for all external communications - Role-based access
control for administrative functions</p>
<h2 data-number="1.9" id="limitations-and-future-work"><span
class="header-section-number">1.9</span> 8. Limitations and Future
Work</h2>
<h3 data-number="1.9.1" id="current-limitations"><span
class="header-section-number">1.9.1</span> 8.1 Current Limitations</h3>
<ol type="1">
<li><strong>Model Size Constraints</strong>: Limited to models that fit
in available memory</li>
<li><strong>Language Support</strong>: Currently optimized for English
content</li>
<li><strong>Specialized Domains</strong>: May require domain-specific
fine-tuning for optimal accuracy</li>
<li><strong>Concurrent Processing</strong>: Limited by hardware
resources for simultaneous requests</li>
</ol>
<h3 data-number="1.9.2" id="future-enhancements"><span
class="header-section-number">1.9.2</span> 8.2 Future Enhancements</h3>
<ol type="1">
<li><strong>Model Quantization</strong>: Implementing model compression
for reduced memory usage</li>
<li><strong>Multi-Language Support</strong>: Extending to additional
languages and cultural contexts</li>
<li><strong>Active Learning</strong>: Incorporating user feedback for
continuous improvement</li>
<li><strong>Federated Deployment</strong>: Enabling distributed
processing across multiple nodes</li>
</ol>
<h2 data-number="1.10" id="conclusion"><span
class="header-section-number">1.10</span> 9. Conclusion</h2>
<p>This paper presents a novel architecture for AI-powered content
curation that addresses key limitations in existing systems. The
pluggable design enables runtime adaptation to varying performance and
accuracy requirements, while BAML integration provides type-safe AI
interactions. Local deployment ensures complete privacy preservation
without sacrificing functionality.</p>
<p>The measured performance characteristics demonstrate practical
applicability, with processing times suitable for real-world deployment.
The comprehensive multi-cloud deployment capabilities indicate
production readiness and scalability.</p>
<p>Future work will focus on expanding language support, implementing
model optimization techniques, and incorporating user feedback
mechanisms for continuous improvement.</p>
<h2 data-number="1.11" id="references"><span
class="header-section-number">1.11</span> References</h2>
<p>[1] Microsoft Content Moderator. “Content Moderator Documentation.”
Microsoft Azure, 2023.</p>
<p>[2] Perspective API. “Perspective API Documentation.” Google Jigsaw,
2023.</p>
<p>[3] Chase, H. “LangChain: Building applications with LLMs through
composability.” GitHub, 2022.</p>
<p>[4] Microsoft. “Semantic Kernel: Integrate cutting-edge LLM
technology quickly and easily.” GitHub, 2023.</p>
<p>[5] McMahan, B., Moore, E., Ramage, D., Hampson, S., &amp; y Arcas,
B. A. “Communication-efficient learning of deep networks from
decentralized data.” AISTATS, 2017.</p>
<p>[6] Dwork, C., &amp; Roth, A. “The algorithmic foundations of
differential privacy.” Foundations and Trends in Theoretical Computer
Science, 2014.</p>
<p>[7] Hard, A., et al. “Federated learning for mobile keyboard
prediction.” arXiv preprint arXiv:1811.03604, 2018.</p>
<h2 data-number="1.12" id="appendix-a-baml-schema-definitions"><span
class="header-section-number">1.12</span> Appendix A: BAML Schema
Definitions</h2>
<p>Complete BAML schema definitions for content analysis, user context,
and response structures are available in the project repository.</p>
<h2 data-number="1.13" id="appendix-b-performance-benchmarks"><span
class="header-section-number">1.13</span> Appendix B: Performance
Benchmarks</h2>
<p>Detailed performance benchmarks including processing time
distributions, memory usage patterns, and concurrent request handling
capabilities are documented in the supplementary materials.</p>
<h2 data-number="1.14" id="appendix-c-deployment-instructions"><span
class="header-section-number">1.14</span> Appendix C: Deployment
Instructions</h2>
<p>Step-by-step deployment instructions for local development and cloud
production environments, including prerequisite software, configuration
parameters, and troubleshooting guides.</p>
<hr />
<p><strong>Author Information:</strong> This work was conducted as part
of the AI Curation Engine project, with complete source code and
documentation available at:
https://github.com/gitmujoshi/ai-curation-engine</p>
<p><strong>Funding:</strong> This research was conducted independently
without external funding.</p>
<p><strong>Conflicts of Interest:</strong> The authors declare no
conflicts of interest.</p>
<p><strong>Data Availability:</strong> The system implementation,
including all source code, configuration files, and deployment scripts,
is available under open source license in the project repository.</p>
</body>
</html>
