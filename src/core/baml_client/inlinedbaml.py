# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "content_classification.baml": "// AI Curation Engine - Content Classification Functions\n// Unified BAML definitions for safety, educational, and viewpoint analysis\n\n// Generator for Python client\ngenerator python_client {\n  output_type \"python/pydantic\"\n  output_dir \"../baml_client_python\"\n  version \"0.208.5\"\n}\n\n// Client configurations\nclient<llm> GPT4 {\n  provider \"openai\"\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> Claude {\n  provider \"anthropic\"\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> Ollama {\n  provider \"openai\"\n  options {\n    model \"llama3.2:latest\"\n    base_url \"http://localhost:11434/v1\"\n    api_key \"ollama\"\n    temperature 0.1\n    max_tokens 800\n    default_role \"user\"\n  }\n}\n\n// Enums for user context\nenum AgeCategory {\n  UNDER_13\n  UNDER_16 \n  UNDER_18\n  ADULT\n}\n\nenum Jurisdiction {\n  EU\n  US\n  INDIA\n  CHINA\n}\n\nenum ParentalControls {\n  NONE\n  MILD\n  MODERATE\n  STRICT\n}\n\nenum SensitivityLevel {\n  LOW\n  MEDIUM\n  HIGH\n}\n\n// Data structures\nclass UserContext {\n  age_category AgeCategory\n  jurisdiction Jurisdiction\n  parental_controls ParentalControls\n  sensitivity_level SensitivityLevel\n}\n\nclass SafetyClassification {\n  safety_score float @description(\"Overall safety score from 0.0 to 1.0\")\n  violence_level float @description(\"Violence content level from 0.0 to 1.0\")\n  adult_content bool @description(\"Contains adult/sexual content\")\n  hate_speech float @description(\"Hate speech detection score from 0.0 to 1.0\")\n  misinformation_risk float @description(\"Risk of misinformation from 0.0 to 1.0\")\n  age_appropriateness string @description(\"Recommended minimum age\")\n  reasoning string @description(\"Explanation of the safety assessment\")\n  content_warnings string[] @description(\"List of specific content warnings\")\n}\n\nclass EducationalValue {\n  educational_score float @description(\"Educational value score from 0.0 to 1.0\")\n  learning_objectives string[] @description(\"Identified learning objectives\")\n  subject_areas string[] @description(\"Academic subject areas covered\")\n  cognitive_level string @description(\"Cognitive complexity level\")\n  reading_level int @description(\"Grade-level reading difficulty\")\n  factual_accuracy float @description(\"Factual accuracy assessment from 0.0 to 1.0\")\n  reasoning string @description(\"Explanation of the educational assessment\")\n}\n\nclass ViewpointAnalysis {\n  political_leaning string @description(\"Political leaning (left, right, neutral)\")\n  bias_score float @description(\"Overall bias score from 0.0 to 1.0\")\n  perspective_diversity float @description(\"Diversity of perspectives from 0.0 to 1.0\")\n  controversy_level float @description(\"Level of controversy from 0.0 to 1.0\")\n  source_credibility float @description(\"Source credibility from 0.0 to 1.0\")\n  reasoning string @description(\"Explanation of the viewpoint assessment\")\n}\n\nclass ComprehensiveClassification {\n  safety SafetyClassification\n  educational EducationalValue\n  viewpoint ViewpointAnalysis\n  overall_confidence float @description(\"Overall confidence score from 0.0 to 1.0\")\n  recommendation string @description(\"Final recommendation: allow, caution, or block\")\n  summary_reasoning string @description(\"Summary of the comprehensive analysis\")\n}\n\n// Functions for content classification\nfunction ClassifySafety(content: string, user_context: UserContext) -> SafetyClassification {\n  client Ollama\n  prompt #\"\n    You are an expert content safety analyst. Analyze the following content for safety and appropriateness.\n\n    Content to analyze:\n    \"\"\"\n    {{ content }}\n    \"\"\"\n\n    User Context:\n    - Age Category: {{ user_context.age_category }}\n    - Jurisdiction: {{ user_context.jurisdiction }}\n    - Parental Controls: {{ user_context.parental_controls }}\n    - Sensitivity Level: {{ user_context.sensitivity_level }}\n\n    Consider the user's context and provide a comprehensive safety analysis.\n    Focus on violence, adult content, hate speech, and misinformation.\n    Provide specific content warnings and age-appropriate recommendations.\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction ClassifyEducational(content: string) -> EducationalValue {\n  client Ollama\n  prompt #\"\n    Evaluate the educational value of the following content.\n\n    Content to analyze:\n    \"\"\"\n    {{ content }}\n    \"\"\"\n\n    Assess the educational merit, learning objectives, subject areas,\n    cognitive complexity, reading level, and factual accuracy.\n    Consider how effective this content would be for learning.\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction ClassifyViewpoint(content: string) -> ViewpointAnalysis {\n  client Ollama\n  prompt #\"\n    Analyze the viewpoint and potential bias in the following content.\n\n    Content to analyze:\n    \"\"\"\n    {{ content }}\n    \"\"\"\n\n    Evaluate political leaning, bias levels, perspective diversity,\n    controversy, and source credibility. Provide a balanced assessment\n    that helps users understand the content's perspective.\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction ComprehensiveContentAnalysis(content: string, user_context: UserContext) -> ComprehensiveClassification {\n  client Ollama\n  prompt #\"\n    Analyze this content and return ONLY valid JSON. No explanations, no disclaimers, just JSON.\n\n    Content: {{ content }}\n    Age: {{ user_context.age_category }}\n\n    {{ ctx.output_format }}\n\n    Rules:\n    - Return ONLY the JSON object above\n    - For mature content: safety_score < 0.5, adult_content = true, recommendation = \"block\"  \n    - For safe content: safety_score > 0.7, recommendation = \"allow\"\n    - Always include reasoning in the reasoning fields\n  \"#\n}",
}

def get_baml_files():
    return _file_map