# üöÄ AI Curation Engine - Complete Local Deployment Guide

## Overview

This comprehensive guide provides step-by-step instructions to deploy the AI Curation Engine locally with **real BAML integration**, **no fallbacks**, and **automated deployment scripts**. Perfect for development, testing, and privacy-focused deployments using local Llama models.

## üìã Prerequisites

### System Requirements
- **OS**: macOS, Linux, or Windows with WSL2
- **RAM**: Minimum 8GB, Recommended 16GB+
- **Storage**: 20GB+ free space for models
- **CPU**: Modern multi-core processor
- **GPU**: Optional but recommended for faster inference

### Software Requirements
- **Python 3.8+** (for the application)
- **pip3** (Python package manager)
- **Git** (for cloning repository)
- **curl** (for testing endpoints)

## üèóÔ∏è Updated Architecture

```mermaid
graph TB
    subgraph "Local Deployment (2024)"
        DEMO[üéØ Demo Frontend<br/>Flask:5001]
        ENGINE[ü§ñ Curation Engine<br/>Pluggable Strategies]
        BAML[üîÑ BAML Integration<br/>Real Implementation]
        OLLAMA[ü¶ô Ollama Server<br/>:11434]
    end
    
    subgraph "Curation Strategies"
        LLM[üß† LLM-Only Strategy<br/>5-10s processing]
        MULTI[‚ö° Multi-Layer Strategy<br/>0.1-5s processing]
        HYBRID[üéØ Hybrid Strategy<br/>Adaptive]
    end
    
    subgraph "Local AI Models"
        LLAMA32[üß† Llama 3.2<br/>7B Parameters]
    end
    
    DEMO --> ENGINE
    ENGINE --> LLM
    ENGINE --> MULTI
    ENGINE --> HYBRID
    BAML --> OLLAMA
    OLLAMA --> LLAMA32
    LLM --> BAML
    MULTI --> BAML
    HYBRID --> BAML
```

## üõ†Ô∏è Quick Start (Automated Deployment)

### Step 1: Clone Repository
```bash
git clone https://github.com/gitmujoshi/ai-curation-engine.git
cd ai-curation-engine
```

### Step 2: One-Command Deployment
```bash
# Make scripts executable
chmod +x *.sh

# Deploy everything automatically
./deploy_local.sh
```

**That's it!** The script will:
- ‚úÖ Check all dependencies
- ‚úÖ Install Python packages
- ‚úÖ Start Ollama with Llama 3.2
- ‚úÖ Generate BAML client
- ‚úÖ Start demo frontend on port 5001
- ‚úÖ Provide complete status report

## üì± Available URLs (After Deployment)

### üéØ Main Application
- **üè† Demo UI**: `http://localhost:5001/`
- **üß™ Content Tester**: `http://localhost:5001/content-test` ‚≠ê
- **‚ù§Ô∏è Health Check**: `http://localhost:5001/health`

### üîß API Endpoints
- **ü§ñ Content Classification**: `POST http://localhost:5001/api/classify`
- **‚öôÔ∏è Strategy Management**: `GET/POST http://localhost:5001/api/strategy`
- **üë• Child Profiles**: `GET http://localhost:5001/api/children`

### ü¶ô Local AI Infrastructure
- **üîó Ollama API**: `http://localhost:11434`
- **üìã Models List**: `http://localhost:11434/api/tags`

## üéÆ Testing Your Deployment

### Quick Health Check
```bash
# Check all services
./status_check.sh

# Or manual check
curl http://localhost:5001/health | jq
```

### Test Content Classification
```bash
# Test with educational content
curl -X POST http://localhost:5001/api/classify \
     -H "Content-Type: application/json" \
     -d '{"content": "This educational content teaches science concepts", "childId": "child_1"}' | jq

# Test strategy switching
curl -X POST http://localhost:5001/api/strategy \
     -H "Content-Type: application/json" \
     -d '{"strategy": "multi_layer"}' | jq
```

### Interactive Testing
1. Open `http://localhost:5001/content-test`
2. Try different content samples
3. Switch between curation strategies
4. Monitor processing times and results

## üöÄ Management Scripts

### Deployment & Control
```bash
./deploy_local.sh     # üöÄ Start everything
./status_check.sh     # üîç Check all services
./stop_services.sh    # üõë Stop all services
./build_and_test.sh   # üèóÔ∏è Validate and test build
```

### What Each Script Does

#### `deploy_local.sh`
- Checks Python, pip, and Ollama dependencies
- Sets up environment variables
- Starts Ollama with Llama 3.2 model
- Generates BAML client
- Starts demo frontend
- Provides complete service status

#### `status_check.sh`
- Tests all service endpoints
- Shows process information
- Validates API functionality
- Provides quick test commands
- Displays all available URLs

#### `stop_services.sh`
- Gracefully stops all services
- Cleans up PID files
- Optionally keeps Ollama running

#### `build_and_test.sh`
- Validates code syntax
- Tests core functionality
- Checks for remaining fallbacks
- Generates documentation

## üéØ Curation Strategies Explained

### 1. LLM-Only Strategy (`llm_only`)
- **Purpose**: Maximum accuracy with comprehensive analysis
- **Processing Time**: 5-10 seconds
- **Best For**: Complex content requiring detailed reasoning
- **Use Case**: High-stakes content moderation

### 2. Multi-Layer Strategy (`multi_layer`)
- **Purpose**: Performance optimization with selective LLM usage
- **Processing Time**: 0.1-5 seconds (adaptive)
- **Layers**: Fast filters ‚Üí Specialized AI ‚Üí LLM (if needed)
- **Best For**: High-volume content processing

### 3. Hybrid Strategy (`hybrid`)
- **Purpose**: Intelligent strategy selection
- **Processing Time**: Variable (1-8 seconds)
- **Logic**: Chooses optimal strategy based on content complexity
- **Best For**: Balanced speed/accuracy requirements

## üîß Manual Installation (If Needed)

### Install Dependencies
```bash
# macOS with Homebrew
brew install python ollama

# Ubuntu/Debian
sudo apt update
sudo apt install python3 python3-pip curl

# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh
```

### Install Python Packages
```bash
# Create requirements.txt
cat > requirements.txt << 'EOF'
flask==2.3.3
flask-cors==4.0.0
requests==2.31.0
pydantic==2.4.2
python-dotenv==1.0.0
baml-py>=0.46.0
EOF

# Install packages
pip3 install -r requirements.txt
```

### Setup Ollama
```bash
# Start Ollama
ollama serve &

# Download Llama 3.2 (one-time setup)
ollama pull llama3.2

# Verify installation
ollama list
```

### Generate BAML Client
```bash
# Install BAML CLI (if needed)
pip3 install baml-py

# Generate client
baml-cli generate
```

### Start Services Manually
```bash
# Terminal 1: Ollama (if not running)
ollama serve

# Terminal 2: Demo Frontend
cd demo-frontend
python3 app.js
```

## üìä What's New in This Version

### ‚úÖ Removed All Fallbacks
- **No mock data**: Pure BAML integration
- **No hardcoded values**: Real AI analysis only
- **Error handling**: Graceful failures without fake responses

### ‚úÖ Pluggable Architecture
- **Three strategies**: LLM-Only, Multi-Layer, Hybrid
- **Runtime switching**: Change strategies without restart
- **Performance monitoring**: Real-time metrics

### ‚úÖ Enhanced UI
- **Strategy control panel**: Switch and monitor strategies
- **Performance display**: Processing times and confidence
- **Sample highlighting**: Better user experience
- **Real-time results**: No cached/static responses

### ‚úÖ Complete Automation
- **One-command deployment**: `./deploy_local.sh`
- **Comprehensive monitoring**: `./status_check.sh`
- **Clean shutdown**: `./stop_services.sh`
- **Build validation**: `./build_and_test.sh`

## üîç Monitoring and Debugging

### Log Files
```bash
# View frontend logs
tail -f logs/demo-frontend.log

# View Ollama logs
tail -f logs/ollama.log

# View BAML generation logs
tail -f logs/baml-generate.log
```

### Common Issues

#### Health Check Fails
```bash
# Check if frontend is running
curl http://localhost:5001/health

# Restart if needed
./stop_services.sh && ./deploy_local.sh
```

#### Slow Classification (>15 seconds)
```bash
# Check Ollama status
curl http://localhost:11434/api/tags

# Restart Ollama if needed
ollama serve
```

#### Strategy Switching Fails
```bash
# Check current strategy
curl http://localhost:5001/api/strategy

# Verify available strategies
curl http://localhost:5001/api/strategy | jq '.available_strategies'
```

## üõ†Ô∏è Customization

### Add New Content Samples
Edit `demo-frontend/app.js` and add to `DEMO_DATA['sample_content']`:
```python
'your_category': {
    'title': 'Your Sample Title',
    'content': 'Your sample content here...'
}
```

### Modify Curation Strategies
Edit `curation_engine_pluggable.py` to customize:
- Filter thresholds in `FastFilterLayer`
- LLM triggers in `_requires_llm_analysis`
- Specialized AI logic in `SpecializedAILayer`

### Environment Configuration
Create `.env` file:
```bash
BAML_LOG_LEVEL=ERROR
BOUNDARY_TELEMETRY_DISABLED=true
OLLAMA_BASE_URL=http://localhost:11434
```

## üöÄ Production Considerations

### Performance Optimization
```bash
# Use GPU if available
ollama serve --gpu

# Increase parallel processing
export OLLAMA_NUM_PARALLEL=4

# Optimize for your CPU
export OLLAMA_NUM_THREAD=8
```

### Security Hardening
- Run behind reverse proxy (nginx)
- Enable HTTPS with SSL certificates
- Implement authentication/authorization
- Configure firewall rules
- Monitor resource usage

### Scaling Options
- **Horizontal**: Multiple Ollama instances with load balancer
- **Vertical**: More RAM/CPU for larger models
- **Hybrid**: Cloud fallback for peak loads

## üìà Expected Performance

### Processing Times (Local Hardware)
- **LLM-Only**: 5-10 seconds (full analysis)
- **Multi-Layer**: 0.1-5 seconds (adaptive)
- **Hybrid**: 1-8 seconds (intelligent selection)

### Resource Usage
- **RAM**: 8-16GB (depending on model size)
- **CPU**: High during inference
- **Storage**: 10-20GB for models
- **Network**: Local only (no external calls)

## üìö Documentation Reference

- **Complete URLs**: `APP_URLS_COMPLETE.md`
- **API Documentation**: `API_ENDPOINTS.md`
- **Architecture Details**: `PRODUCTION_CURATION_ARCHITECTURE.md`
- **BAML Implementation**: `BAML_IMPLEMENTATION_STATUS.md`
- **Strategy Demo Guide**: `STRATEGY_DEMO_GUIDE.md`

## üéâ Quick Success Verification

After running `./deploy_local.sh`, you should see:

```
‚úÖ Demo Frontend: Running (port 5001)
‚úÖ Ollama: Running (port 11434)
‚úÖ Health Check: Working
‚úÖ Classification API: Working (7+s processing time)
‚úÖ Strategy API: Working
‚úÖ BAML Integration: Available
‚úÖ No Fallbacks: Pure BAML implementation
```

**üöÄ Start testing**: Open `http://localhost:5001/content-test`

---

This deployment guide reflects the latest updates with **no fallbacks**, **automated deployment**, **pluggable strategies**, and **comprehensive monitoring**. The setup prioritizes **real AI analysis**, **performance flexibility**, and **ease of use** while maintaining privacy with local-only processing.